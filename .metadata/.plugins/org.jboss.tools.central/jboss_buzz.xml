<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Network testing with testpmd and noisy_vnf</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/21/network-testing-testpmd-and-noisyvnf" /><author><name>Mike Pattrick</name></author><id>f3a11ccf-dc3e-4f1e-b869-305dee44ed80</id><updated>2023-09-21T07:00:00Z</updated><published>2023-09-21T07:00:00Z</published><summary type="html">&lt;p&gt;Developers of software-defined network (SDN) frameworks and applications often use the &lt;a href="dpdk.org"&gt;DPDK&lt;/a&gt; utility &lt;a href="https://doc.dpdk.org/guides/testpmd_app_ug/"&gt;testpmd&lt;/a&gt; to test DPDK features and benchmark network hardware performance. In the upcoming DPDK release &lt;a href="http://doc.dpdk.org/guides/rel_notes/release_23_07.html"&gt;23.07&lt;/a&gt;, we've extended the functionality of the &lt;code&gt;noisy_vnf&lt;/code&gt; module in &lt;code&gt;testpmd&lt;/code&gt; to allow better simulations of &lt;a href="https://www.redhat.com/en/blog/openshift-enterprise-production"&gt;heavily loaded servers&lt;/a&gt; or &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes"&gt;complex workloads&lt;/a&gt;. This can allow for more realistic &lt;a href="https://access.redhat.com/articles/6969629"&gt;benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The default DPDK forwarding mode is &lt;code&gt;io&lt;/code&gt;, which reads a batch of packets from an Rx queue and writes those packets directly to a Tx queue. Because the packets aren't processed or modified, the throughput achieved with this forwarding mode could be considered a theoretical maximum and not likely to be reproduced by real-world applications. Other forwarding modes like &lt;code&gt;macswap&lt;/code&gt; and &lt;code&gt;5tswap&lt;/code&gt; process and modify layer 2 and 3, respectively; however, they do not perform any other per packet or batch processing. These forwarding modes are described in more detail in the DPDK &lt;a href="https://doc.dpdk.org/guides/testpmd_app_ug/testpmd_funcs.html#set-fwd"&gt;testpmd documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;noisy_vnf&lt;/code&gt; module has existed in DPDK since 18.11, but its usefulness has been limited by the inability to combine its use with other forwarding modes like &lt;code&gt;macswap&lt;/code&gt; or &lt;code&gt;5tswap&lt;/code&gt;. Instead, it has only forwarded packets unmodified like the io module does. This has hindered its inclusion in complex simulations. With DPDK 23.07, &lt;code&gt;noisy_vnf&lt;/code&gt; users will be able to select various forwarding modes, including &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;macswap&lt;/code&gt;, and &lt;code&gt;5tswap&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Network configuration&lt;/h2&gt; &lt;p&gt;To illustrate how this can be used, I've configured a small network with a simple application that sends and receives UDP packets; this application measures the round trip time (RTT) between send and receive by embedding a timestamp in the packet on send and comparing that timestamp to the current time receipt. The network path takes traffic out of a physical network interface into a second interface through &lt;a href="http://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt;, and finally, to &lt;code&gt;testpmd&lt;/code&gt;. See Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/noisy.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/noisy.png?itok=7Xt9-f3a" width="600" height="656" alt="Network topology" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Network topology&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The use of Open vSwitch could allow for a variety of overlay networks to quickly be swapped in or out. For example, a VLAN or &lt;a href="https://www.redhat.com/en/blog/what-geneve"&gt;GENEVE&lt;/a&gt; VPN could easily be configured. But in this example, the default &lt;a href="https://docs.openvswitch.org/en/latest/ref/ovs-actions.7/#the-ovs-normal-pipeline"&gt;Normal&lt;/a&gt; action is left in place; this causes Open vSwitch to act like a traditional switch.&lt;/p&gt; &lt;p&gt;The client application was configured to write a timestamp into the packet that it sent and then compare that with the current time when received to get an accurate measure of end-to-end RTT.&lt;/p&gt; &lt;h2&gt;Software configuration&lt;/h2&gt; &lt;p&gt;We ran three separate tests to show how &lt;code&gt;noisy_vnf&lt;/code&gt; parameters, as described below, can affect transmission rates.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;noisy-forward-mode&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; The &lt;code&gt;noisy_vnf&lt;/code&gt; forwarding mode; currently only &lt;code&gt;io&lt;/code&gt;, &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;macswap&lt;/code&gt;, and &lt;code&gt;5tswap&lt;/code&gt; are supported.&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-tx-sw-buffer-size&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; Allocates a FIFO packet queue in number of packets. Ingress packets fill this buffer until it is full or until a flush time has expired.&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-tx-sw-buffer-flushtime&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; Flush time, in milliseconds, for the FIFO packet buffer&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-lkup-memory&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; The amount of memory to allocate – in MB – for random read/write activity.&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;noisy-lkup-num-reads&lt;/code&gt;, &lt;code&gt;noisy-lkup-num-writes&lt;/code&gt;, &lt;code&gt;noisy-lkup-num-reads-writes&lt;/code&gt;: The number of reads and/or writes to conduct in the memory buffer allocated from &lt;code&gt;noisy-lkup-memory&lt;/code&gt; per packet.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The first test only used the &lt;code&gt;5tswap&lt;/code&gt; forwarding mode. Next, &lt;code&gt;noisy_vnf&lt;/code&gt; was configured to perform 200 random reads and 20 random writes across 400 MB of memory. Finally, &lt;code&gt;noisy_vnf&lt;/code&gt; was configured to perform 32 random reads and 8 random writes across a 200 MB buffer and to buffer 128 packets in a FIFO pipeline. The actual &lt;code&gt;testpmd&lt;/code&gt; command-line invocations are included below.&lt;/p&gt; &lt;p&gt;The first set of &lt;code&gt;noisy_vnf&lt;/code&gt; parameters attempts to simulate an application that requires a lot of processing per packet, including accessing memory that may have fallen out of the CPU cache; for example, traversing a large data structure like nested hash tables. The second set of parameters attempts to simulate an application that has to queue up multiple packets before processing them all at once. The per-packet read and write rates are much lower but, in aggregate, add up to a larger amount of memory activity per batch of packets.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# 5tswap run dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --rxq 4 --forward-mode=5tswap&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# noisy_vnf run 1 dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --noisy-tx-sw-buffer-size=0 --noisy-tx-sw-buffer-flushtime=0 \ --noisy-lkup-memory=400 --noisy-lkup-num-writes=20 --noisy-lkup-num-reads=200 \ --noisy-forward-mode=5tswap --rxq 4 --forward-mode=noisy&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# noisy_vnf run 2 dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --noisy-tx-sw-buffer-size=128 --noisy-tx-sw-buffer-flushtime=20 \ --noisy-lkup-memory=200 --noisy-lkup-num-writes=8 --noisy-lkup-num-reads=32 \ --noisy-forward-mode=5tswap --rxq 4 --forward-mode=noisy&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# OVS configurations 34a5a4cc-69e2-46e5-abed-8a73e5b08dd7 Bridge noisytest datapath_type: netdev Port dpdk0 Interface dpdk0 type: dpdkvhostuserclient options: {n_rxq="4", vhost-server-path="/tmp/vhost0"} Port noisytest Interface noisytest type: internal Port dpdk1 Interface dpdk1 type: dpdk options: {dpdk-devargs="0000:03:00.1", n_rxq="4", n_rxq_desc="4096", n_txq_desc="4096"} ovs_version: "3.0.90"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The client application used for this test is available on &lt;a href="https://github.com/mkp-rh/pitcher"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;p&gt;The table below shows performance results in packets per second and microsecond round trip time while only changing the &lt;code&gt;testpmd&lt;/code&gt; invocation.&lt;/p&gt; &lt;h3&gt;5tswap test&lt;/h3&gt; &lt;p&gt;Packet source and destination addresses swapped.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second:&lt;/strong&gt; 145,000&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time:&lt;/strong&gt; 134&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;noisy_vnf 1 test&lt;/h3&gt; &lt;p&gt;Source and destination swapped; 220 memory operations over 400 MB of memory per batch.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second: &lt;/strong&gt;40,000&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time:&lt;/strong&gt; 235&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;noisy_vnf 2 test&lt;/h3&gt; &lt;p&gt;Source and destination swapped; 40 memory operations over 200 MB of memory per batch; and 128 packets queued in a FIFO buffer.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second: &lt;/strong&gt;992&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time: &lt;/strong&gt;20,225&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;These results demonstrate how substantially a packet buffer or a lot of random memory access can impact packet throughput. Merely inserting a few hundred memory operations with each packet saw a near doubling of round trip time. Using a FIFO packet buffer had a 40x reduction in packets per second throughput. These results make it clear that a network speed test that doesn't involve any per packet processing will not give a complete picture of the performance capabilities of the network setup.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/21/network-testing-testpmd-and-noisyvnf" title="Network testing with testpmd and noisy_vnf"&gt;Network testing with testpmd and noisy_vnf&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mike Pattrick</dc:creator><dc:date>2023-09-21T07:00:00Z</dc:date></entry><entry><title>Automate your AMQ streams platform with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/20/automate-your-amq-streams-platform-ansible" /><author><name>Roman Martin Gil</name></author><id>d56fa280-2f57-4d50-9460-febc163988e3</id><updated>2023-09-20T07:00:00Z</updated><published>2023-09-20T07:00:00Z</published><summary type="html">&lt;p&gt;Nowadays, &lt;a href="http://developers.redhat.com/topics/automation"&gt;IT automation&lt;/a&gt; is a must to accelerate, improve and deliver value in a secured, tested, and easy way. &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; has become the go-to tool for IT teams due to its simplicity, versatility, and powerful automation capabilities. With its agentless architecture, Ansible allows for seamless deployment, configuration management, and orchestration across a wide range of systems and platforms.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kuberneteshttp://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; has emerged as the preferred tool for handling real-time data streams. Its distributed architecture, fault tolerance, and scalability make it a robust choice for building &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architecture (EDA)&lt;/a&gt;. &lt;a href="https://access.redhat.com/products/red-hat-amq-streams/"&gt;Red Hat AMQ streams&lt;/a&gt; is the enterprise edition of this platform provided by Red Hat. &lt;/p&gt; &lt;p&gt;Both amazing tools meet together in the &lt;a href="https://github.com/ansible-middleware"&gt;Ansible Middleware&lt;/a&gt; community, providing the &lt;a href="https://galaxy.ansible.com/middleware_automation/amq_streams"&gt;Ansible collection for Red Hat AMQ streams&lt;/a&gt;. This collection includes a set of automation capabilities and features to manage and operate Apache Kafka clusters on top of &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to easily deploy an Apache Kafka cluster based on AMQ streams using the Ansible collection for Red Hat AMQ streams.&lt;/p&gt; &lt;h2&gt;Install the collection&lt;/h2&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To make use of this tutorial, you need a Red Hat Enterprise Linux or Fedora system, along with version 2.12 or higher of Ansible (preferably the latest version).&lt;/p&gt; &lt;p&gt;The very first step, of course, is to install the collection itself so that Ansible can use its content inside playbooks:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.amq_streams&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before going further, you should check to make sure that the collection has been successfully installed. To do so, run the following command from Ansible Galaxy that will list all the installed collections:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection list  Collection                          Version ----------------------------------- ------- ansible.posix 1.5.4 middleware_automation.amq_streams   0.0.5   &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; As this collection is under development and evolution, the version downloaded might differ from the version displayed.&lt;/p&gt; &lt;p&gt;Now that you've installed the collection and its dependencies, you can use them to automate the installation of AMQ streams.&lt;/p&gt; &lt;h2&gt;Deploy AMQ streams with Ansible&lt;/h2&gt; &lt;p&gt;Thanks to the dedicated Ansible collection for AMQ streams, automating the installation and configuration of Apache Kafka is easy. However, before you implement this collection inside your playbook, we should recap what we mean here by &lt;strong&gt;installing&lt;/strong&gt; and &lt;strong&gt;configuring&lt;/strong&gt; Apache Kafka. Indeed, this task encompasses quite a few operations that are performed on the target system:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creating appropriate OS user and group accounts.&lt;/li&gt; &lt;li aria-level="1"&gt;Downloading the installation archive from the Kafka website.&lt;/li&gt; &lt;li aria-level="1"&gt;Unarchiving the contents while ensuring that all the files are associated with the appropriate user and groups along with the correct permissions.&lt;/li&gt; &lt;li aria-level="1"&gt;Ensuring that the required version of the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; Virtual Machine (JVM) is installed.&lt;/li&gt; &lt;li aria-level="1"&gt;Integrating the software into the host service management service— in our case, the Linux &lt;a href="https://www.linux.com/training-tutorials/understanding-and-using-systemd/"&gt;systemd&lt;/a&gt; daemon.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Apache Kafka is a distributed ecosystem of different components with multiple deployment options and capabilities. The Ansible collection for AMQ streams helps automate most common deployment and configuration topologies. This article shows one of these cases for the purposes of demonstrating the capabilities of the collection. For further information, please refer to the &lt;a href="https://ansiblemiddleware.com/amq_streams/"&gt;Ansible collection for AMQ streams documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates our use case: a distributed cluster formed by a ZooKeeper ensemble of three nodes and three Kafka brokers managed by Ansible.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/amq-streams-ansible.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/amq-streams-ansible.png?itok=Zk08NuOI" width="600" height="427" alt="AMQ Streams deployment managed by Ansible" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: AMQ Streams deployment managed by Ansible&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;All of this is achieved and is fully automated by the following playbook, combining several different roles included within the Ansible collection of AMQ streams.&lt;/p&gt; &lt;p&gt;Create a playbook at the path &lt;code&gt;playbooks/my-amq_streams_distributed.yml&lt;/code&gt; containing the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Ansible Playbook to install a Zookeeper ensemble and Kafka Broker Authenticated" hosts: all vars: # Enabling Zookeeper Authentication amq_streams_zookeeper_auth_enabled: true amq_streams_zookeeper_auth_user: zkadmin amq_streams_zookeeper_auth_pass: p@ssw0rd # Enabling Kafka BrokerListeners amq_streams_broker_listeners: - AUTHENTICATED://:{{ amq_streams_broker_listener_port }} # Authenticated - REPLICATION://:{{ amq_streams_broker_listener_internal_port }} # Inter broker communication # Listener for inter-broker communications amq_streams_broker_inter_broker_listener: REPLICATION # Enabling Kafka Broker Authentication amq_streams_broker_auth_enabled: true amq_streams_broker_auth_scram_enabled: true amq_streams_broker_auth_listeners: - AUTHENTICATED:SASL_PLAINTEXT - REPLICATION:SASL_PLAINTEXT amq_streams_broker_auth_sasl_mechanisms: - PLAIN - SCRAM-SHA-512 # Kafka Plain Users amq_streams_broker_auth_plain_users: - username: admin password: p@ssw0rd - username: kafkauser password: p@ssw0rd # Setting Kafka user for inter-broker communication amq_streams_broker_inter_broker_auth_sasl_mechanisms: PLAIN amq_streams_broker_inter_broker_auth_broker_username: interbroker amq_streams_broker_inter_broker_auth_broker_password: p@ssw0rd # Enabling Broker replication amq_streams_broker_offsets_topic_replication_factor: 3 amq_streams_broker_transaction_state_log_replication_factor: 3 amq_streams_broker_transaction_state_log_min_isr: 2 roles: - role: amq_streams_zookeeper tasks: - name: "Ensure Zookeeper is running and available." ansible.builtin.include_role: name: amq_streams_zookeeper - name: "Ensure AMQ Streams Broker is running and available." ansible.builtin.include_role: name: amq_streams_broker post_tasks: - name: "Display numbers of Zookeeper instances managed by Ansible." ansible.builtin.debug: msg: "Numbers of Zookeeper instances: {{ amq_streams_zookeeper_instance_count }}." when: - amq_streams_zookeeper_instance_count_enabled is defined and amq_streams_zookeeper_instance_count_enabled - name: "Display numbers of broker instances managed by Ansible." ansible.builtin.debug: msg: "Numbers of broker instances: {{ amq_streams_broker_instance_count }}." when: - amq_streams_broker_instance_count_enabled is defined and amq_streams_broker_instance_count_enabled - name: "Validate that Zookeeper deployment is functional." ansible.builtin.include_role: name: amq_streams_zookeeper tasks_from: validate.yml - name: "Validate that Broker deployment is functional." ansible.builtin.include_role: name: amq_streams_broker tasks_from: validate.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook includes a set of different users and their credentials. Note that because these variables contain sensitive data, they should be secured using &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;Ansible Vault&lt;/a&gt; or some other secrets management system. However, that task is beyond the scope of this article.&lt;/p&gt; &lt;p&gt;The inventory of hosts to deploy the desired topology looks similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;[all] rhel9mw01 rhel9mw02 rhel9mw03 [zookeepers] rhel9mw01 rhel9mw02 rhel9mw03 [brokers] rhel9mw01 rhel9mw02 rhel9mw03 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new file called &lt;code&gt;inventory&lt;/code&gt; with the contents above.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;zookeepers&lt;/code&gt; group identifies the hosts to deploy the ZooKeeper component, and the &lt;code&gt;brokers&lt;/code&gt; group identifies the hosts to deploy the Kafka brokers.&lt;/p&gt; &lt;p&gt;Run this playbook as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory playbooks/my-amq_streams_distributed.yml&lt;/code&gt;&lt;/pre&gt; &lt;div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In order for this playbook to perform the installation outlined here, Ansible must have &lt;code&gt;sudo&lt;/code&gt; or root privileges on the target hosts.&lt;/p&gt; &lt;h2&gt;Check for successful installation&lt;/h2&gt; &lt;p&gt;Once the playbook finishes its execution, you can confirm that the ZooKeeper and Broker services are now running by verifying their status.&lt;/p&gt; &lt;h3&gt;ZooKeeper&lt;/h3&gt; &lt;p&gt;Check the ZooKeeper service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[rhmw@rhel9mw01 logs]$ sudo systemctl status amq_streams_zookeeper.service ● amq_streams_zookeeper.service - amq_streams_zookeeper Loaded: loaded (/usr/lib/systemd/system/amq_streams_zookeeper.service; enabled; preset: disabled) Drop-In: /usr/lib/systemd/system/service.d └─10-timeout-abort.conf Active: active (running) since Wed 2023-06-28 15:16:03 CEST; 3min 59s ago Main PID: 4873 (java) Tasks: 52 (limit: 2298) Memory: 125.1M CPU: 3.567s CGroup: /system.slice/amq_streams_zookeeper.service └─4873 java -Xmx256M -Xms256M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true "-Xlog&gt; Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,050] INFO Committing global session 0x300000721bc0000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,320] INFO Committing global session 0x10000064a600000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,390] INFO Successfully authenticated client: authenticationID=zkadmin; authorizationID=zkadmin. (org.apache.zookeeper.server.auth.Sas&gt; Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,455] INFO Setting authorizedID: zkadmin (org.apache.zookeeper.server.auth.SaslServerCallbackHandler) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,458] INFO adding SASL authorization for authorizationID: zkadmin (org.apache.zookeeper.server.ZooKeeperServer) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,543] INFO Committing global session 0x300000721bc0001 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:48 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:48,086] INFO Submitting global closeSession request for session 0x10000064a600000 (org.apache.zookeeper.server.ZooKeeperServer) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,524] INFO Committing global session 0x300000721bc0002 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,650] INFO Committing global session 0x200000647ab0000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,816] INFO Committing global session 0x300000721bc0003 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Kafka broker&lt;/h3&gt; &lt;p&gt;Check the Kafka broker service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhmw@rhel9mw01 logs]$ sudo systemctl status amq_streams_broker.service ● amq_streams_broker.service - amq_streams_broker Loaded: loaded (/usr/lib/systemd/system/amq_streams_broker.service; enabled; preset: disabled) Drop-In: /usr/lib/systemd/system/service.d └─10-timeout-abort.conf Active: active (running) since Wed 2023-06-28 15:16:48 CEST; 5min ago Main PID: 11241 (java) Tasks: 78 (limit: 2298) Memory: 319.1M CPU: 8.601s CGroup: /system.slice/amq_streams_broker.service └─11241 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true "-Xlog:gc&gt; Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,104] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,208] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,240] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,251] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka startTimeMs: 1687958212256 (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,258] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,335] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node rhel9mw01:9091 (i&gt; Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,383] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node rhel9mw01:909 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the Kafka cluster&lt;/h2&gt; &lt;p&gt;So, our Apache Kafka cluster is up and running and ready to go and manage streaming events. Let's test it by creating a simple topic, producing some messages, and consuming them.&lt;/p&gt; &lt;p&gt;This test can be accomplished using the Kafka CLI scripts below for these actions. These commands are simple examples and can be extended to implement more complex actions in your environment.&lt;/p&gt; &lt;p&gt;As our Kafka cluster requires authentication, we need to create a file with the user credentials and authentication mechanism to reference when invoking the CLI. In our case, this file will use the &lt;code&gt;kafkauser&lt;/code&gt; user created by the playbook.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat &lt;&lt;EOF &gt; /tmp/kafka-cli.properties security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="kafkauser" password="password"; EOF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will create the &lt;code&gt;sample-topic&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --command-config /tmp/kafka-cli.properties --create --topic sample-topic --partitions 10 --replication-factor 3 Created topic sample-topic. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;kafka-console-producer.sh&lt;/code&gt; script allows for sending messages to a topic. The following actions will publish messages to the topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sample-topic --producer.config /tmp/kafka-cli.properties &gt;Hello Ansible Collection for AMQ Streams!!!! &gt;Another message!! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With a series of messages published, the &lt;code&gt;kafka-console-consumer.sh&lt;/code&gt; script can be used to consume the messages. The following command will enable the consumption of the messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sample-topic --consumer.config /tmp/kafka-cli.properties --from-beginning --timeout-ms 10000 Hello Ansible Collection for AMQ Streams!!!! Another message!! [2023-06-28 16:04:24,896] ERROR Error processing message, terminating consumer process: (kafka.tools.ConsoleConsumer$) org.apache.kafka.common.errors.TimeoutException Processed a total of 2 messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cool! Your Apache Kafka cluster is ready and fully automated by Ansible.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;By using Ansible and the Ansible collection for AMQ streams as outlined in this article, you can fully automate the deployment of an event streaming platform in any RHEL-based environment without any manual intervention. Ansible performed all the heavy lifting (downloading software, preparing the OS, creating users and groups, deploying the binary files and the configuration, setting up the service, and more) and even setting up the cluster with a set of users.&lt;/p&gt; &lt;p&gt;The Ansible collection for AMQ streams allows you to streamline the installation and configuration of Apache Kafka, thus enabling you to scale deployments as necessary and ensure repeatability across them all.&lt;/p&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/20/automate-your-amq-streams-platform-ansible" title="Automate your AMQ streams platform with Ansible"&gt;Automate your AMQ streams platform with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Roman Martin Gil</dc:creator><dc:date>2023-09-20T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.4.1 released - Redis 7.2 and Flyway changes</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-4-1-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-4-1-released/</id><updated>2023-09-20T00:00:00Z</updated><published>2023-09-20T00:00:00Z</published><summary type="html">It is our pleasure to announce the release of Quarkus 3.4.1. We skipped 3.4.0 as we needed a fix for CVE-2023-4853 in 3.4 too. Major changes are: Support for Redis 7.2 Adjustments on how to enable/activate Flyway This version also comes with bugfixes, performance improvements and documentation improvements. We currently...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-20T00:00:00Z</dc:date></entry><entry><title>Write operators in Java with JOSDK, Part 4: Upgrading strategies</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/19/write-operators-java-josdk-part-4-upgrading-strategies" /><author><name>Christophe Laprun</name></author><id>5a7e37cf-e472-4465-8b2f-4226bbc93d45</id><updated>2023-09-19T07:00:00Z</updated><published>2023-09-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt;(JOSDK) is an open source project that aims to simplify the task of creating Kubernetes operators using Java. &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt; started the project, and Red Hat is now a major contributor. The JOSDK project now lives under the &lt;a href="https://github.com/operator-framework"&gt;Operator Framework umbrella&lt;/a&gt;, which is a &lt;a href="https://cncf.io"&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; incubating project.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk"&gt;first article in this series&lt;/a&gt; introduced JOSDK and explained why it could be interesting to create operators in Java. The &lt;a href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2"&gt;second article&lt;/a&gt; showed how the &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;JOSDK Quarkus extension &lt;code&gt;quarkus-operator-sdk&lt;/code&gt;&lt;/a&gt;, also called QOSDK, facilitates the development experience by taking care of managing the Custom Resource Definition automatically. The &lt;a href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller"&gt;third article&lt;/a&gt; focused on requirements for implementing the reconciliation logic for the example operator you build in this series. Many things have changed since the third installment of this series. This article will thus focus on updating the code to the latest versions and provide upgrading strategies.&lt;/p&gt; &lt;h2&gt;Where things stand&lt;/h2&gt; &lt;p&gt;You implemented a simple operator exposing your application outside the cluster via an &lt;code&gt;Ingress&lt;/code&gt;, creating the associated &lt;code&gt;Deployment&lt;/code&gt; and &lt;code&gt;Service&lt;/code&gt; along the way. However, it has been a while since the last part of this blog series and many things have changed. When the third article was written, QOSDK was in version 3.0.4. Now it is up to 6.3.0. Quarkus has also been updated. How can you update your operator to use more recent versions, and what are possible strategies to update your code?&lt;/p&gt; &lt;h2&gt;How to use Quarkus update&lt;/h2&gt; &lt;p&gt;Upgrading a project is always a tricky proposition, especially when there is a wide gap between the old and new versions. Quarkus can help you with this task, though it might not work in all cases. In this case, you want to migrate from Quarkus 2.7.3.Final to the latest version, which at the time of writing this article, is 3.2.4.Final. You can use the &lt;code&gt;update&lt;/code&gt; command that Quarkus provides. If you have the &lt;code&gt;quarkus&lt;/code&gt; command line tool, you might want to upgrade it first and then simply run &lt;code&gt;quarkus update&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Otherwise, using maven only, you can run:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;mvn io.quarkus.platform:quarkus-maven-plugin:3.2.4.Final:update -N&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The complete procedure is detailed in the &lt;a href="https://quarkus.io/guides/update-quarkus"&gt;related Quarkus guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In your case, you should notice that the update procedure fails with an error when the command attempts to check the updated project as follows:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;[INFO] [ERROR] [ERROR] Some problems were encountered while processing the POMs: [INFO] [ERROR] 'dependencies.dependency.version' for io.quarkiverse.operatorsdk:quarkus-operator-sdk-csv-generator:jar is missing. @ line 38, column 17&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Updating outdated QOSDK dependency&lt;/h2&gt; &lt;p&gt;The problem occurs because this dependency doesn’t exist anymore. Though the project actually doesn’t need this dependency at this point, it is included by default when bootstrapping a QOSDK project using the &lt;code&gt;operator-sdk&lt;/code&gt; CLI and allows for automatic generation of &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt; bundles. OLM enables you to manage the lifecycle of operators on clusters in a more principled way. We might discuss this feature in greater detail in a future article.&lt;/p&gt; &lt;p&gt;There are two ways to fix your project. If you’re not interested in the feature, you can remove the dependency, or change it to the correct one. This dependency doesn’t exist in its previous form anymore because it has been renamed to better reflect its expanded scope. It initially focused solely on the &lt;a href="https://olm.operatorframework.io/docs/concepts/crds/clusterserviceversion/"&gt;&lt;code&gt;ClusterServiceVersion&lt;/code&gt;&lt;/a&gt; part of OLM bundles, but now extends to generating complete bundles. The feature was actually disabled using &lt;code&gt;quarkus.operator-sdk.generate-csv=false&lt;/code&gt; in the &lt;code&gt;application.properties&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;The new dependency name is &lt;code&gt;quarkus-operator-sdk-bundle-generator&lt;/code&gt;. So use that if you want to use the OLM generation feature. Note that you will also need to change the associated property name to activate the feature. You’ll see a warning in the logs that the property doesn’t exist if you don’t, and the OLM generation will be activated by default. The new property is named &lt;code&gt;quarkus.operator-sdk.bundle.enabled&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After making these changes, you can re-run the update command. It should now succeed, with an output similar to the following:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;[INFO] Detected project Java version: 11 [INFO] Quarkus platform BOMs: [INFO] io.quarkus:quarkus-bom:pom:3.2.4.Final ✔ [INFO] Add: io.quarkus.platform:quarkus-operator-sdk-bom:pom:3.2.4.Final [INFO] [INFO] Extensions from io.quarkus:quarkus-bom: [INFO] io.quarkus:quarkus-micrometer-registry-prometheus ✔ [INFO] [INFO] Extensions from io.quarkus.platform:quarkus-operator-sdk-bom: [INFO] Update: io.quarkiverse.operatorsdk:quarkus-operator-sdk-bundle-generator:6.3.0 -&gt; remove version (managed) [INFO] Update: io.quarkiverse.operatorsdk:quarkus-operator-sdk:6.3.0 -&gt; remove version (managed)&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Strategies for QOSDK and Quarkus updates&lt;/h2&gt; &lt;p&gt;Now you can see that you can actually simplify things even further. It is advising you to add the &lt;code&gt;io. quarkus.platform:quarkus-operator-sdk-bom:pom:3.2.4.Final&lt;/code&gt; dependency. Indeed, QOSDK has been added to the Quarkus platform, making it easier to consume from a given Quarkus version. Switching to this BOM only allows you to decide which version of Quarkus to use, and the BOM will make sure you get the appropriate QOSDK version.&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="xml"&gt;&lt;strong&gt;&lt;dependencyManagement&gt;&lt;/strong&gt; &lt;strong&gt;&lt;dependencies&gt;&lt;/strong&gt; &lt;strong&gt;&lt;dependency&gt;&lt;/strong&gt; &lt;strong&gt;&lt;groupId&gt;&lt;/strong&gt;io.quarkiverse.operatorsdk&lt;strong&gt;&lt;/groupId&gt;&lt;/strong&gt; &lt;strong&gt;&lt;artifactId&gt;&lt;/strong&gt;quarkus-operator-sdk-bom&lt;strong&gt;&lt;/artifactId&gt;&lt;/strong&gt; &lt;strong&gt;&lt;version&gt;&lt;/strong&gt;${quarkus-sdk.version}&lt;strong&gt;&lt;/version&gt;&lt;/strong&gt; &lt;strong&gt;&lt;scope&gt;&lt;/strong&gt;import&lt;strong&gt;&lt;/scope&gt;&lt;/strong&gt; &lt;strong&gt;&lt;type&gt;&lt;/strong&gt;pom&lt;strong&gt;&lt;/type&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependency&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependencies&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependencyManagement&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This project is currently using the QOSDK BOM with &lt;code&gt;quarkus-sdk.version&lt;/code&gt; with the 3.0.4 value. You’ll also note that there is a &lt;code&gt;quarkus.version&lt;/code&gt; property with the 2.7.3.Final value. Looking at the QOSDK BOM, you can see that there is also a Quarkus version property defined there, with the same &lt;code&gt;quarkus.version&lt;/code&gt; name. Therefore, if you upgrade the QOSDK version, with the current setup, you need to make sure to also upgrade the Quarkus version in your project in such a way that is compatible with the version defined in the QOSDK BOM.&lt;/p&gt; &lt;p&gt;Using the QOSDK BOM defined by the Quarkus platform (i.e., Using the &lt;code&gt;io.quarkus.platform:quarkus-operator-sdk-bom&lt;/code&gt; artifact instead of the &lt;code&gt;io.quarkiverse.operatorsdk:quarkus-operator-sdk-bom&lt;/code&gt;, note the different group identifier.) simplifies this aspect by making sure that both QOSDK and Quarkus versions are aligned. The downside of this is that using the QOSDK BOM directly from the QOSDK project, you have the Quarkus BOM automatically included in your project. The price for this, as previously explained, is that you need to make sure the versions are in sync.&lt;/p&gt; &lt;p&gt;That said, you can also see that it is letting us know that there is a more recent version of the QOSDK extension (6. 3.0), which will only be available from the Quarkus platform starting with version 3.2.5.Final. Using the Quarkus platform, therefore, means that you’re not necessarily using the latest QOSDK version. This is, however, the version that is verified to work with the platform as a whole, so this is the more conservative option.&lt;/p&gt; &lt;p&gt;If you wish to use the absolute latest version of QOSDK, you should use the BOM provided by QOSDK, but you will need to make sure to update the Quarkus version using the &lt;code&gt;quarkus.version&lt;/code&gt;, while updating the QOSDK version using the &lt;code&gt;quarkus-sdk.version&lt;/code&gt; property in your &lt;code&gt;pom.xml&lt;/code&gt; file as previously done.&lt;/p&gt; &lt;p&gt;Which approach to choose depends on your appetence for risk, or how you wish to manage your dependencies. Generally speaking, the Quarkus platform is updated frequently, and QOSDK versions are usually updated accordingly as needed. So the Quarkus platform is usually up-to-date when it comes to the latest QOSDK version. If you absolutely need the latest QOSDK version, upgrading from the Quarkus platform offerings, a patch or even a minor version should typically work with issues since QOSDK strives to maintain backwards compatibility between minor versions.&lt;/p&gt; &lt;p&gt;Going the opposite direction, upgrading Quarkus to a minor version above (e.g., from 3.2.x to 3.3.x) might prove tricky since the Fabric8 Kubernetes client version used by that new Quarkus version might also have been updated to a new minor version. This has been known to bring API changes, so you might want to tread carefully with such updates.&lt;/p&gt; &lt;p&gt;Actually, QOSDK issues debug-level warnings when it detects version mismatches (minor version and above, patch level mismatches considered safe) between Quarkus, JOSDK, and Fabric8 Kubernetes client. You can even configure it to fail a build by setting the &lt;code&gt;quarkus.operator-sdk.fail-on-version-check&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;. Please refer to the &lt;a href="https://docs.quarkiverse.io/quarkus-operator-sdk/dev/index.html#quarkus-operator-sdk_quarkus.operator-sdk.fail-on-version-check"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;It’s also worth repeating that since QOSDK bundles JOSDK, you do not need to worry about updating that dependency separately. One less thing to worry about.&lt;/p&gt; &lt;h2&gt;Adapting to Fabric8 Kubernetes client changes&lt;/h2&gt; &lt;p&gt;Now that the dependencies are sorted out, if you try to build now, you should get a compilation error due to an API change in the Fabric8 Kubernetes client:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:compile (&lt;strong&gt;default&lt;/strong&gt;-compile) on project expose: Compilation failure [ERROR] exposedapp-rhdblog/src/main/java/io/halkyon/ExposedAppReconciler.java:[63,33] cannot find symbol [ERROR] symbol: method withIntVal(&lt;strong&gt;int&lt;/strong&gt;) [ERROR] location: &lt;strong&gt;interface&lt;/strong&gt; &lt;strong&gt;io&lt;/strong&gt;.fabric8.kubernetes.api.model.ServicePortFluent.TargetPortNested&lt;io.fabric8.kubernetes.api.model.ServiceSpecFluent.PortsNested&lt;io.fabric8.kubernetes.api.model.ServiceFluent.SpecNested&lt;io.fabric8.kubernetes.api.model.ServiceBuilder&gt;&gt;&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This issue is easily fixed by changing the following line:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;.withNewTargetPort().withIntVal(8080).endTargetPort()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;.withNewTargetPort(8080)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Fabric8 Kubernetes client provides detailed notes for each release. It’s always a good idea to take a look at them, especially whenever a new minor version is released (&lt;a href="https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0"&gt;here&lt;/a&gt; are the notes for the 6.8. 0 release, which does contain breaking changes). Another interesting resource is the &lt;a href="https://github.com/fabric8io/kubernetes-client/blob/main/doc/CHEATSHEET.md"&gt;cheat sheet&lt;/a&gt;, which contains a wealth of information on how to perform a wide variety of tasks using the client.&lt;/p&gt; &lt;p&gt;That said, you should now be all set for this batch of updates!&lt;/p&gt; &lt;h2 id="_conclusion"&gt;Summary&lt;/h2&gt; &lt;p&gt;While less focused on writing operators per se, this article still covered an important part of any software development: upgrading dependencies. Your operator should be now ready for improvements, which we will tackle in the next article. We will also discuss adding status handling and how to make your operator react to events that are not targeting primary resources.&lt;/p&gt; &lt;p&gt;For reference, you can find the completed code for this part under the &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog/tree/part-4"&gt;&lt;code&gt;part-4&lt;/code&gt; tag&lt;/a&gt; of the &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog"&gt;https://github.com/halkyonio/exposedapp-rhdblog&lt;/a&gt; repository.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/19/write-operators-java-josdk-part-4-upgrading-strategies" title="Write operators in Java with JOSDK, Part 4: Upgrading strategies"&gt;Write operators in Java with JOSDK, Part 4: Upgrading strategies&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2023-09-19T07:00:00Z</dc:date></entry><entry><title>When Quarkus meets Virtual Threads</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/virtual-thread-1/&#xA;            " /><author><name>Clement Escoffier (https://twitter.com/clementplop)</name></author><id>https://quarkus.io/blog/virtual-thread-1/</id><updated>2023-09-19T00:00:00Z</updated><published>2023-09-19T00:00:00Z</published><summary type="html">Java 21 offers a new feature that will reshape the development of concurrent applications in Java. For over two years, the Quarkus team explored integrating this new feature to ease the development of distributed applications, including microservices and event-driven applications. This blog post is the first part of a series...</summary><dc:creator>Clement Escoffier (https://twitter.com/clementplop)</dc:creator><dc:date>2023-09-19T00:00:00Z</dc:date></entry><entry><title>A statistics update in Open vSwitch user space datapath</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/18/statistics-update-open-vswitch-user-space-datapath" /><author><name>David Marchand</name></author><id>31406c88-48e4-4cec-a6b6-9095aa82356d</id><updated>2023-09-18T07:00:00Z</updated><published>2023-09-18T07:00:00Z</published><summary type="html">&lt;p&gt;With the demands for higher bandwidth, came the need for scaling and processing packets on more CPU resources. In Open vSwitch (OVS) using DPDK for faster IO, this translated to using more receive and transmit queues to allow more PMD threads to process the packets. This adds some complexity to a system not easy to understand in the first place. Support or operation people still want to know how much traffic is received and how it is distributed across the CPU resources. To offer help, this article describes new statistics added for the user space datapath in OVS 2.17 and later.&lt;/p&gt; &lt;h2&gt;Per queue statistics for DPDK ports&lt;/h2&gt; &lt;p&gt;A first evolution in OVS 2.17 consisted of &lt;a href="https://github.com/openvswitch/ovs/commit/1140c87e2eb7"&gt;exposing receive and transmit queues statistics&lt;/a&gt; per DPDK physical ports in ovsdb.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface dpdk0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep packets= | grep -v '=0$' rx_packets=5553474 rx_q0_packets=3705290 rx_q1_packets=1848184 tx_broadcast_packets=220 tx_multicast_packets=488 tx_packets=39406658924 tx_q1_packets=3700644 tx_q2_packets=97 tx_q3_packets=19696490438 tx_q4_packets=19706467745&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Those per queue statistics require support from the DPDK driver backing the port.&lt;/p&gt; &lt;p&gt;A vast majority of physical (and even some virtual) NIC DPDK drivers do support those statistics. But if no statistics appear in ovsdb, you may check for support by looking for the RTE_ETH_DEV_AUTOFILL_QUEUE_XSTATS (1 &lt;&lt; 6) value in the port dev_flags bitmask through the DPDK telemetry tool (coming with the dpdk-tools rpm).&lt;/p&gt; &lt;p&gt;For example, check port 0:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# if [ $(($(echo /ethdev/info,0 | dpdk-telemetry.py -f /var/run/openvswitch/dpdk/rte | jq -r '.["/ethdev/info"]["dev_flags"]') &amp; 64)) != 0 ]; then echo per queue stats are supported; else echo per queue stats may not be implemented for this driver; fi per queue stats are supported&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Per queue statistics for vhost-user ports&lt;/h2&gt; &lt;p&gt;Getting the same level of information for vhost-user ports has required some reworking in the DPDK vhost-user libary because the library was not accounting such information.&lt;/p&gt; &lt;p&gt;&lt;br /&gt; This was enhanced by the community in the DPDK v22.07 release with this &lt;a href="https://git.dpdk.org/dpdk/commit/?id=be75dc99ea1f"&gt;change&lt;/a&gt;, and its support was merged in OVS with this &lt;a href="https://github.com/openvswitch/ovs/commit/3b29286db1c5"&gt;change&lt;/a&gt; in the 3.1 version:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface vhost0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep packets= | grep -v '=0$' rx_65_to_127_packets=2987595 rx_packets=2987595 rx_q0_good_packets=2987595 rx_q0_size_65_127_packets=2987595 tx_65_to_127_packets=14075727 tx_packets=14075727 tx_q0_good_packets=14075727 tx_q0_size_65_127_packets=14075727&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;More vhost-user statistics&lt;/h2&gt; &lt;p&gt;As a bonus of the work exposing per queue statistics, the vhost-user library started exposing other internal counters.&lt;/p&gt; &lt;p&gt;The virtio driver (e.g., the Linux kernel driver by default) plugged on a vhost-user port may require guest notifications for signaling packets delivery. Triggering those notifications impacts the processing cost of such packets, which is why keeping track of the amount of notifications is of interest.&lt;/p&gt; &lt;p&gt;Previously, OVS was exposing a coverage counter for those notifications, and until OVS 3.0, you could use the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-appctl coverage/show | grep vhost_notification vhost_notification 0.0/sec 0.000/sec 2.0283/sec total: 7302&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This coverage counter was only hinting at some vhost-user ports used by an unidentified virtual machine.&lt;/p&gt; &lt;p&gt;Starting OVS 3.1, the coverage counter has been removed in favor of per queue and per port statistics (&lt;a href="https://git.dpdk.org/dpdk/commit/?id=1ea74efd7fa4"&gt;DPDK change&lt;/a&gt; / &lt;a href="https://github.com/openvswitch/ovs/commit/c9e10ac57fb8"&gt;OVS change&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface vhost0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep guest_notifications rx_q0_guest_notifications=12 rx_q1_guest_notifications=1 tx_q0_guest_notifications=3 tx_q1_guest_notifications=2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This nice addition makes it possible to directly point at which virtual machine is slowing down packet processing. Other vhost-user statistics have been added, like exposing the &lt;a href="https://git.dpdk.org/dpdk/commit/?id=7247b7464ef9"&gt;vhost-user IOTLB cache internals&lt;/a&gt;. More may be added in the future as members of the community express new requirements.&lt;/p&gt; &lt;h2&gt;A final note about statistics&lt;/h2&gt; &lt;p&gt;As OVS stores per interface statistics in its ovsdb, choices were made to select generic (iow not driver specific) statistics, and that helps in a majority of use cases.&lt;/p&gt; &lt;p&gt;However, if you do not find the driver-specific statistics you're looking for, it is still possible for debugging to use the DPDK telemetry tool and retrieve all unfiltered port statistics as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# echo /ethdev/xstats,0 | dpdk-telemetry.py -f /var/run/openvswitch/dpdk/rte { "/ethdev/xstats": { "rx_good_packets": 5553474, "tx_good_packets": 39406658860, "rx_good_bytes": 710844672, "tx_good_bytes": 4886425719104, "rx_missed_errors": 78892319, "rx_errors": 0, "tx_errors": 0, "rx_mbuf_allocation_errors": 0, "rx_q0_packets": 3705290, "rx_q0_bytes": 474277120, "rx_q0_errors": 0, "rx_q1_packets": 1848184, "rx_q1_bytes": 236567552, "rx_q1_errors": 0, "tx_q0_packets": 0, "tx_q0_bytes": 0, "tx_q1_packets": 3700615, "tx_q1_bytes": 458874621, "tx_q2_packets": 71, "tx_q2_bytes": 8120, "tx_q3_packets": 19696490435, "tx_q3_bytes": 2442364825811, "tx_q4_packets": 19706467739, "tx_q4_bytes": 2443602010552, "rx_wqe_errors": 0, "rx_unicast_packets": 84445793, "rx_unicast_bytes": 10471278332, "tx_unicast_packets": 39406658216, "tx_unicast_bytes": 4886425618784, "rx_multicast_packets": 0, "rx_multicast_bytes": 0, "tx_multicast_packets": 444, "tx_multicast_bytes": 35320, "rx_broadcast_packets": 0, "rx_broadcast_bytes": 0, "tx_broadcast_packets": 200, "tx_broadcast_bytes": 65000, "tx_phy_packets": 39406658860, "rx_phy_packets": 84445793, "rx_phy_crc_errors": 0, "tx_phy_bytes": 5044052354544, "rx_phy_bytes": 10809061504, "rx_phy_in_range_len_errors": 0, "rx_phy_symbol_errors": 0, "rx_phy_discard_packets": 0, "tx_phy_discard_packets": 0, "tx_phy_errors": 0, "rx_out_of_buffer": 78892319, "tx_pp_missed_interrupt_errors": 0, "tx_pp_rearm_queue_errors": 0, "tx_pp_clock_queue_errors": 0, "tx_pp_timestamp_past_errors": 0, "tx_pp_timestamp_future_errors": 0, "tx_pp_jitter": 0, "tx_pp_wander": 0, "tx_pp_sync_lost": 0 } }&lt;/code&gt;&lt;/pre&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/18/statistics-update-open-vswitch-user-space-datapath" title="A statistics update in Open vSwitch user space datapath"&gt;A statistics update in Open vSwitch user space datapath&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Marchand</dc:creator><dc:date>2023-09-18T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #36 - September</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-36/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-36/</id><updated>2023-09-15T00:00:00Z</updated><published>2023-09-15T00:00:00Z</published><summary type="html">Explore how we can use the Testcontainers Desktop app while building a Quarkus application by reading "Joyful Quarkus Application Development using Testcontainers Desktop" by Siva Katamreddy. Extensions can significantly increase the application’s performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2023-09-15T00:00:00Z</dc:date></entry><entry><title>How to configure RHEL as a workstation during installation</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/14/how-configure-rhel-workstation-during-installation" /><author><name>Nikhil Mungale</name></author><id>a410ee41-5f4f-455e-91fe-5183739593cf</id><updated>2023-09-14T07:00:00Z</updated><published>2023-09-14T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) is a powerful and widely-used &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; distribution known for its stability, security features, and enterprise-grade support. When installing RHEL, you have the opportunity to configure it as a workstation to optimize performance and usability for your specific needs. In this article, we will guide you through the steps to configure RHEL as a workstation during the installation process.&lt;/p&gt; &lt;h2&gt;What is RHEL workstation?&lt;/h2&gt; &lt;p&gt;The organization needs high-end workstations that can support developers and power users, such as artists, physicians, scientists, and engineers, so they can concentrate on what they do best. Designed for advanced Linux users and day-to-day usage with powerful hardware, &lt;a href="https://www.redhat.com/en/store/red-hat-enterprise-linux-workstation"&gt;Red Hat Enterprise Linux for Workstations&lt;/a&gt; (RHEL Workstation) is optimized for high-performance graphics, animation, and scientific applications. It includes all the capabilities and applications that workstation users need, plus development tools for provisioning and administration.&lt;/p&gt; &lt;h2&gt;How to install and configure a RHEL workstation&lt;/h2&gt; &lt;p&gt;The following sections will demonstrate how to configure RHEL as a workstation during the installation process.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Red Hat Enterprise Linux installation media or ISO file.&lt;/li&gt; &lt;li aria-level="1"&gt;Bootable USB/DVD drive containing.ISO file of RHEL 9.2.&lt;/li&gt; &lt;li aria-level="1"&gt;Activated no-cost &lt;a href="developer.redhat.com"&gt;Red Hat Developer subscription&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;A system that meets the following requirements: &lt;ul&gt;&lt;li aria-level="2"&gt;A 64-bit x86 or ARM machine&lt;/li&gt; &lt;li aria-level="2"&gt;4 GB of RAM&lt;/li&gt; &lt;li aria-level="2"&gt;At least 20 GB of available disk space(50 GB for best results)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Prepare Windows system for RHEL installation&lt;/h3&gt; &lt;p&gt;We are making a single machine with two operating systems, RHEL workstation and Windows 11. We need to isolate the operating systems from each other to install RHEL on the same disk where Windows OS is already installed.&lt;/p&gt; &lt;p&gt;Follow these steps required to make a single disk into multiple partitions:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Right-click on the Windows icon and select the disk management option, as shown in Figure 1.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture1_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture1_0.png?itok=N9SnoOfv" width="596" height="497" alt="Disk management" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1:  Selecting Disk Management.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;You will see the window with all drives shown graphically.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the drive which has enough space to make a partition.&lt;/li&gt; &lt;li aria-level="1"&gt;Right-click on drive and select the &lt;strong&gt;Shrink Volume&lt;/strong&gt; option.&lt;/li&gt; &lt;li aria-level="1"&gt;In the wizard, define the partition and size by entering &lt;strong&gt;Enter the amount of shrink in MB&lt;/strong&gt; and click on the &lt;strong&gt;Shrink&lt;/strong&gt; button. (Note: Define space at least 50GB for best experience.)&lt;/li&gt; &lt;li aria-level="1"&gt;The freed space should now be shown with &lt;strong&gt;Unallocated Status&lt;/strong&gt; in the bottom pane. Right-click on it and select &lt;strong&gt;New Simple Volume&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the next wizard, select &lt;strong&gt;Do not assign a drive letter or drive path &lt;/strong&gt;and click on &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Format Partition&lt;/strong&gt; window, keep everything as default and change the&lt;strong&gt; Volume label&lt;/strong&gt; new volume to &lt;strong&gt;RHEL.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Finish&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;After completing the setup, you will see the disk partition for RHEL in Figure 2.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture2_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture2_0.png?itok=-Hbehbyh" width="600" height="257" alt="disk" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Disk partition for RHEL OS&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Start installing RHEL&lt;/h3&gt; &lt;p&gt;To begin installation on RHEL, make sure a bootable USB/DVD is attached to the device.&lt;/p&gt; &lt;p&gt;Press the shift&lt;strong&gt; &lt;/strong&gt;key and restart&lt;strong&gt; &lt;/strong&gt;the system, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture3_0.png?itok=VGosab6M" width="493" height="436" alt="restart" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Restart the system&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;During system boot up, the screen in Figure 4 appears. Click on the&lt;strong&gt; Use a device &lt;/strong&gt;option.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture4_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture4_0.png?itok=02Yixx_C" width="600" height="370" alt="boot option" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4:  Choosing a boot option.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Select the&lt;strong&gt; Linpus lite (USB&lt;/strong&gt;) option using navigation keys from the keyboard and press enter, as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture5_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture5_0.png?itok=pGca53f4" width="600" height="405" alt="USB" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Select the USB boot device&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The screen in Figure 6 appears to choose the RHEL installation option, and boot the system using bootable installation media containing the RHEL 9.iso file. For this article, we will use &lt;strong&gt;Red Hat Enterprise Linux 9.2.&lt;/strong&gt; Then press enter.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture6_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture6_0.png?itok=4v1Vvns4" width="600" height="453" alt="rhel select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Select RHEL to install.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: During booting, you can skip the media checking step by hitting the &lt;strong&gt;Esc&lt;/strong&gt; key.&lt;/p&gt; &lt;p&gt;As shown in Figure 7, select the language of RHEL to install and click &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture7_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture7_0.png?itok=e1blA1Zw" width="600" height="449" alt="language select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Selecting the OS language.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure RHEL&lt;/h3&gt; &lt;p&gt;During installation, we must manually configure RHEL.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Connect to Red Hat&lt;/strong&gt;, as shown in Figure 8.&lt;/li&gt; &lt;li aria-level="1"&gt;Fill in your username&lt;strong&gt; &lt;/strong&gt;and password&lt;strong&gt; &lt;/strong&gt;for the activated no-cost Red Hat Developer subscription.&lt;/li&gt; &lt;li aria-level="1"&gt;Leave the organization blank (optional).&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Register &lt;/strong&gt;button.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture8.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture8.png?itok=Df1CXQvA" width="600" height="426" alt="config select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: The RHEL configuration screen. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Choose the disk and partition&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;Installation Destination&lt;/strong&gt; has a caution sign. To install RHEL, we must select the drive. There are two ways to install RHEL on disk with a dedicated disk and partitioned disk space.&lt;/p&gt; &lt;h4&gt;Dedicated disk&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Installation Destination&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;It shows all available disks. Select the disk on which you want to install RHEL.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Partitioned disk&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Get into the &lt;strong&gt;Installation Destination&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on disk and select the claim drive.&lt;/li&gt; &lt;li aria-level="1"&gt;In the window in Figure 9, you will see all the partitions of the hard disk.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;RHEL&lt;/strong&gt; labeled drive.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Delete &lt;/strong&gt;to the left.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the &lt;strong&gt;Reclaim space&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture9.png?itok=vdhsbR1r" width="600" height="450" alt="disk select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Choose the disk for OS installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the network and host name&lt;/h3&gt; &lt;p&gt;Next, configure the system network.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Network &amp; HostName&lt;/strong&gt; under &lt;strong&gt;SYSTEM&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Turn on the adaptor by clicking the toggle switch.&lt;/li&gt; &lt;li aria-level="1"&gt;If you have built-in WIFI in the system, it shows the Ethernets below as a secondary option to connect to the internet.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you do not enable the network here, you will not be able to install RHEL, because it downloads all dependencies over the internet during the installation process.&lt;/p&gt; &lt;h3&gt;Set the root password&lt;/h3&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Scroll down to the &lt;strong&gt;Root Password&lt;/strong&gt; under the user setting and click on it.&lt;/li&gt; &lt;li aria-level="1"&gt;Next, add the password two times. Make sure it is a strong password. Based on the complexity of your password, it will show the strength of the password under it.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Select the software&lt;/h3&gt; &lt;p&gt;RHEL gives the flexibility to install additional packages and tools during the installation process so users can get most of the things in a single go. Refer to Figure 10 for the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Software Selection&lt;/strong&gt; button.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the &lt;strong&gt;Workstation &lt;/strong&gt;under the &lt;strong&gt;Base Environment&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;From &lt;strong&gt;Additional software for Selected Environment,&lt;/strong&gt; select the tools of your preference, such as container management, security tools, etc.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture10.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture10.png?itok=WCJRDTIL" width="600" height="458" alt="select workstation" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: Selecting workstation as the RHEL version. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Install the RHEL Workstation&lt;/h3&gt; &lt;p&gt;After completing all configurations, we are ready to begin the installation of RHEL Workstation.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Click the &lt;strong&gt;Begin installation&lt;/strong&gt; button shown in Figure 11 to start the installation.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture11.png?itok=2h5gV_Nt" width="600" height="450" alt="all config done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11:  Begin the installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The installation may take some time based on your internet speed and compute power.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;After installation, we need to reboot the system, by clicking on the &lt;strong&gt;Reboot System&lt;/strong&gt; button in Figure 12.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture12.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture12.png?itok=Vv5vSrrd" width="600" height="443" alt="installation done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: Rebooting the system.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Set up RHEL&lt;/h3&gt; &lt;p&gt;After the successful installation of RHEL, you will see the screen in Figure 13.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Simply click &lt;strong&gt;Start Setup&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture13.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture13.png?itok=HwquSu0y" width="600" height="456" alt="rhel setup" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13:  Start the RHEL setup.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;RHEL requires more inputs before using it.&lt;/li&gt; &lt;li aria-level="1"&gt;Accept the policy of RHEL and click on next.&lt;/li&gt; &lt;li aria-level="1"&gt;Add the accounts of Google and Microsoft or skip it and click on Next.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a new user ID (Figure 14). Fill in the &lt;strong&gt;Full Name&lt;/strong&gt; and &lt;strong&gt;Username &lt;/strong&gt;boxes and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture14.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture14.png?itok=tQPQ7HZ9" width="600" height="445" alt="Create a user" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 14: Create a user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;On the next screen, set the password for the login system. Make sure the password is strong.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Start Using Red Hat Enterprise Linux&lt;/strong&gt; (Figure 15).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture15.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture15.png?itok=Cf5G3uA8" width="600" height="450" alt="click on start" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 15: Click the Start Using Red Hat Enterprise Linux button.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;/ul&gt;&lt;p&gt;RHEL Workstation is now ready for personal and high-end use (Figure 16).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture16.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture16.png?itok=GB0cpuQS" width="600" height="449" alt="workstation done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 16: The desktop view of RHEL 9.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;If you want to have a more hands-on experience of RHEL, you can follow the &lt;a href="https://www.redhat.com/en/interactive-labs/enterprise-linux"&gt;Red Hat curated lab&lt;/a&gt;. Learn more with Red Hat's hands-on labs for all skill levels. Try these labs to see your favorite products in action.&lt;/p&gt; &lt;p&gt;You can also &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;get customized RHEL images&lt;/a&gt; for AWS, Google Cloud Platform, Microsoft Azure, and VMware and deploy them to the platform of your choice.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/14/how-configure-rhel-workstation-during-installation" title="How to configure RHEL as a workstation during installation"&gt;How to configure RHEL as a workstation during installation&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale</dc:creator><dc:date>2023-09-14T07:00:00Z</dc:date></entry><entry><title>Quarkus security releases for CVE-2023-4853</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/cve-2023-4853/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/cve-2023-4853/</id><updated>2023-09-14T00:00:00Z</updated><published>2023-09-14T00:00:00Z</published><summary type="html">We have just released updates to Quarkus 2.16.11.Final, 3.2.6.Final, and 3.3.3 and Red Hat build of Quarkus 2.13.18.SP2 that fix the issue reported in CVE-2023-4853. This issue affects anyone using HTTP security path-based rules to protect HTTP endpoints. Recommendations If you are using any older versions of Quarkus (ranging from...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-14T00:00:00Z</dc:date></entry><entry><title>Introducing Ansible Molecule with Ansible Automation Platform</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/13/introducing-ansible-molecule-ansible-automation-platform" /><author><name>Anshul Behl</name></author><id>dd391abe-15a5-43e1-9cfd-d6b0b32bf624</id><updated>2023-09-13T07:00:00Z</updated><published>2023-09-13T07:00:00Z</published><summary type="html">&lt;p&gt;Ansible Molecule is a tool designed to aid in developing and testing Ansible playbooks, roles, and collections. It provides support for functional testing of Ansible content across multiple instances, operating systems and distributions, virtualization providers, test frameworks, and testing scenarios. Molecule helps Ansible content creators (&lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; specialists) consistently deliver automation content that is scalable, repeatable, and compatible with the latest Ansible versions.&lt;/p&gt; &lt;p&gt;Ansible Molecule 6 is now available as a &lt;a href="https://access.redhat.com/support/offerings/devpreview"&gt;developer preview&lt;/a&gt; with &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. This version will refocus and redefine the project as a tool for testing Ansible content with Ansible Automation Platform.&lt;/p&gt; &lt;p&gt;The developer preview enables us to collect feedback from our users as we work towards making it an integral and supported part of the Ansible Automation Platform developer experience. This release is part of our broader strategy to reduce the learning curve required for IT professionals and Ansible specialists with little to no coding skills to build, test, and deploy their automation content.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note to current Molecule users:&lt;/strong&gt; If you are already familiar with the Molecule project and are using it to test your automation content, there might be breaking changes that will require updates to your test scenarios. Please see the conclusion section of this blog to find out how to provide feedback to us.&lt;/p&gt; &lt;h2&gt;An automation testing framework built for the enterprise&lt;/h2&gt; &lt;p&gt;The latest Ansible Molecule developer preview is designed with Ansible Automation Platform and organizational level testing in mind. While it retains its core goal of providing a reliable way to make sure your automation is up to scratch, it's got some new tricks up its sleeve. Now, you can also test roles and playbooks within &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_collections.html"&gt;Ansible Content Collections&lt;/a&gt;, making it even easier to develop and validate your automation content.&lt;/p&gt; &lt;p&gt;This update comes thanks to valuable feedback from our community of Molecule users. We've listened and made Molecule more user-friendly, especially for those who create and specialize in Ansible automation.&lt;/p&gt; &lt;p&gt;Here's a rundown of what's new and improved.&lt;/p&gt; &lt;h2&gt;Testing framework for content inside Ansible Content Collections&lt;/h2&gt; &lt;p&gt;Ansible Content Collections are a distribution format for Ansible content that can include playbooks, roles, modules, and plug-ins. They are used to distribute reusable Ansible content, enabling users to share, version, and distribute the building blocks of their automation.&lt;/p&gt; &lt;p&gt;Recognizing the complexity and interdependence of today's automation tasks, we have extended the capabilities of Molecule to test not just individual roles and playbooks but entire collections. This enhancement is particularly significant as we prepare for a seamless integration of Molecule into the Ansible Automation Platform.&lt;/p&gt; &lt;p&gt;The integration with Ansible Automation Platform positions Molecule as a future-ready tool. More importantly, it aligns with a user-friendly strategy for content testing within the platform. This approach aims to simplify the user experience, enabling both customers and community users to conduct comprehensive tests on their automation content in a manner that is consistent with the Ansible Automation Platform.&lt;/p&gt; &lt;h2&gt;Keeping it simple with one driver&lt;/h2&gt; &lt;p&gt;Ansible is now the default provisioner with this Molecule release. Molecule uses drivers as provisioners to create the infrastructure to run your tests on. Prior to the release of version 6, Molecule supported multiple drivers to provision testing instances using different technologies, including &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; containers, virtual machines, and cloud providers. By default, it came with three pre-installed drivers: Docker and Podman drivers to manage &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and a delegated driver allowing you to customize your integration using Ansible. Drivers for other providers were available through the open source community.&lt;/p&gt; &lt;p&gt;The default and the only driver present with Ansible Molecule in Ansible Automation Platform is the delegated driver (aliased as "default" driver), which allows you to use Ansible itself to create and modify how Molecule provisions its test environments.&lt;/p&gt; &lt;p&gt;Ansible can automate various environments using the Ansible collections available through the community and Ansible Automation Platform. Because the delegated ("default") driver uses Ansible, we believe that it will help the adoption of Molecule as the testing framework for the enterprise.&lt;/p&gt; &lt;p&gt;Although using other drivers was a powerful approach that allowed customizations, this approach had its disadvantages. For instance, if you want to customize a driver's provisioning/de-provisioning mechanism, you will need to change the driver's source code, which means that you will need to go through the learning curve of writing Python code even if you are an Ansible playbook writer.&lt;/p&gt; &lt;h2&gt;Making the testinfra verifier optional&lt;/h2&gt; &lt;p&gt;In the context of Ansible Molecule, a "verifier" is a component responsible for running tests against the infrastructure or instances created during the testing process. The verifier is used to validate whether the Ansible role or playbook being tested has successfully achieved the desired state on the test instances. The verifier you choose determines the testing framework and syntax you use to write your tests.&lt;/p&gt; &lt;p&gt;Ansible is a powerful verifier and has been made the default verifier with Molecule 6. Testinfra is another popular verifier with Molecule, which has been made optional with Molecule 6. The testinfra library requires Molecule users to be proficient with &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, which limits its usability for many non-programmer IT practitioners. Making the Testinfra library optional with Molecule 6 is part of our efforts to refocus Molecule as a tool for functional testing of Ansible roles, collections, and playbooks using Ansible itself. Writing verifications can be done with native Ansible playbooks and tasks rather than using/learning a third-party tool with respect to Python.&lt;/p&gt; &lt;p&gt;What we mean by optional is that the testinfra Python library is not packaged as part of Molecule 6 for the downstream release. Rather, the testing "glue" is still available for testinfra in Molecule. For those IT practitioners who may be comfortable with Python and wish to use testinfra, it remains an installable option.&lt;/p&gt; &lt;h2&gt;Installing Molecule developer preview&lt;/h2&gt; &lt;p&gt;Molecule is packaged as part of the Ansible Automation Platform. You can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download the bundled installer&lt;/a&gt; or &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.4/html/red_hat_ansible_automation_platform_planning_guide/proc-attaching-subscriptions_planning"&gt;subscribe to the Ansible Automation Platform repos&lt;/a&gt; to get access to the supported packages.&lt;/p&gt; &lt;p&gt;You can install Molecule using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install \ --enablerepo=ansible-automation-platform-2.4-for-rhel-8-x86_64-rpms molecule&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting started with Molecule developer preview&lt;/h2&gt; &lt;p&gt;Let's take a look at how Molecule developer preview aligns more closely with Ansible content collection development and testing. All the examples here are available in the upstream Molecule &lt;a href="https://ansible.readthedocs.io/projects/molecule/getting-started/"&gt;project documentation&lt;/a&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;One of the recommended ways to create a collection is to place it under the &lt;code&gt;collections/ansible_collections&lt;/code&gt; directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy collection init foo.bar&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;roles&lt;/code&gt; directory in your new collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/foo.bar/roles/&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Initialize a new role for this collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy role init my_role&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add a task under &lt;code&gt;my_role/tasks/main.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--- - name: Task is running from within the role ansible.builtin.debug: msg: "This is a task from my_role." &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add Molecule to the content collection:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Create a new directory in your collection called &lt;code&gt;extensions&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the new &lt;code&gt;extensions&lt;/code&gt; directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Initialize the default Molecule scenario:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;molecule init scenario&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Edit the &lt;code&gt;molecule.yml&lt;/code&gt; file to use your local collection development environment as described. Add the following entry to your &lt;code&gt;&lt;path_to_your_collection&gt;/extensions/molecule/default/molecule.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;provisioner: name: ansible config_options: defaults: collections_path: ${ANSIBLE_COLLECTIONS_PATH}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Then, set the &lt;code&gt;ANSIBLE_COLLECTIONS_PATH&lt;/code&gt; environment variable at the command line before running Molecule:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export ANSIBLE_COLLECTIONS_PATH=/home/user/working/collections&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the path should reflect the location up to the &lt;code&gt;collections&lt;/code&gt; directory and not the &lt;code&gt;ansible_collections&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Molecule scenarios&lt;/h2&gt; &lt;p&gt;Scenarios are the starting point for a lot of powerful functionality that Molecule offers. Think of a scenario as a test suite for roles or playbooks within a collection. You can have as many scenarios as you like, and Molecule will run them sequentially.&lt;/p&gt; &lt;h3&gt;The scenario layout&lt;/h3&gt; &lt;p&gt;Within the &lt;code&gt;molecule/default&lt;/code&gt; folder, we find several files:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ls create.yml destroy.yml molecule.yml converge.yml&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;create.yml&lt;/code&gt; is a playbook file used for creating the instances and storing data in &lt;code&gt;instance-config&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;destroy.yml&lt;/code&gt; has the Ansible code for destroying the instances and removing them from &lt;code&gt;instance-config&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;molecule.yml&lt;/code&gt; is the central configuration entry point for Molecule per scenario. With this file, you can configure each tool that Molecule will employ when testing your role.&lt;/li&gt; &lt;li&gt;&lt;code&gt;converge.yml&lt;/code&gt; is the playbook file that contains the call for your role. Molecule will invoke this playbook with &lt;code&gt;ansible-playbook&lt;/code&gt; and run it against an instance created by the driver.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Inspecting the molecule.yml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;molecule.yml&lt;/code&gt; is for configuring Molecule. It is a &lt;a href="https://yaml.org/"&gt;YAML&lt;/a&gt; file with keys that represent the high-level components that Molecule provides. These are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#dependency"&gt;dependency&lt;/a&gt; manager:&lt;/strong&gt; Molecule uses &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/dev_guide.html"&gt;galaxy development guide&lt;/a&gt; by default to resolve your role dependencies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#platforms"&gt;platforms&lt;/a&gt; definitions:&lt;/strong&gt; Molecule relies on this to know which instances to create and name and which group each instance belongs to. If you need to test your role against multiple popular distributions (&lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS&lt;/a&gt;, Fedora, Debian, &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;), you can specify that in this section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#provisioner"&gt;provisioner&lt;/a&gt;:&lt;/strong&gt; Molecule only provides an Ansible provisioner. Ansible manages the life cycle of the instance based on this configuration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#scenario"&gt;scenario&lt;/a&gt; definition: &lt;/strong&gt;Molecule relies on this configuration to control the scenario sequence order.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#verifier"&gt;verifier&lt;/a&gt; framework:&lt;/strong&gt; Molecule uses Ansible by default to provide a way to write specific state-checking tests (such as deployment smoke tests) on the target instance.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Running a full test sequence&lt;/h3&gt; &lt;p&gt;Molecule provides commands to manually manage the lifecycle of the instance, scenario, development, and testing tools. However, we can also tell Molecule to manage this automatically within a scenario sequence.&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the extensions directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The full life cycle sequence can be invoked with &lt;code&gt;molecule test&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Molecule full lifecycle sequence └── default ├── dependency ├── cleanup ├── destroy ├── syntax ├── create ├── prepare ├── converge ├── idempotence ├── side_effect ├── verify ├── cleanup └── destroy&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Testing the collection role&lt;/h3&gt; &lt;p&gt;One of the default files created as part of the initialization is the &lt;code&gt;converge.yml&lt;/code&gt; file. This file is a playbook created to run your role from start to finish. This can be modified if needed, but is a good place to start if you have never used Molecule before.&lt;/p&gt; &lt;p&gt;You now have an isolated test environment and can also use it for live development by running &lt;code&gt;molecule converge&lt;/code&gt;. It will run through the same steps as above but will stop after the &lt;code&gt;converge&lt;/code&gt; action. Then, you can make changes to your collection or the converge play, and then run &lt;code&gt;molecule converge&lt;/code&gt; again (and again) until you're done with your development work.&lt;/p&gt; &lt;p&gt;We can test the role by adding the following code to &lt;code&gt;converge.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--- - name: Include a role from a collection hosts: localhost gather_facts: false tasks: - name: Testing role ansible.builtin.include_role: name: foo.bar.my_role tasks_from: main.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the extensions directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;molecule converge&lt;/code&gt;&lt;/pre&gt; &lt;div&gt;The above command runs the same steps as the molecule test for the default scenario but will stop after the converge action. This is beneficial if you want to keep the infrastructure up while you are doing your collections development work and testing.&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;By introducing Ansible Molecule as a developer preview as part of an Ansible Automation Platform subscription, we are working towards ensuring the project is stable, supported, and maintainable in an enterprise environment.&lt;/p&gt; &lt;p&gt;If you have any questions or feedback on the changes, please reach out to the &lt;a href="https://github.com/ansible/molecule/issues/new/choose"&gt;Ansible Molecule project on Github&lt;/a&gt;. The project's maintainers will be happy to answer any questions on this topic.&lt;/p&gt; &lt;p&gt;As the project matures and evolves, we will keep you updated with more use cases for Ansible Molecule, including deep dives on testing with multiple operating systems, integrating Molecule with your &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; pipelines, and more.&lt;/p&gt; &lt;h2&gt;Where to go next&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://ansible.readthedocs.io/projects/molecule/"&gt;Molecule project documentation&lt;/a&gt;: Check out the detailed documentation on the upstream molecule project that has a lot of getting started use cases with Molecule.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get hands-on with on-demand Ansible Automation Platform self-paced exercises&lt;/a&gt;. We have a variety of interactive in-browser exercises to experience Ansible Automation Platform in action.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Trial subscription&lt;/a&gt;: Are you ready to install on-premises? Get your own trial subscription for unlimited access to all the components of Ansible Automation Platform.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/ansibleautomation"&gt;Subscribe&lt;/a&gt; to the Red Hat Ansible Automation Platform YouTube channel.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/13/introducing-ansible-molecule-ansible-automation-platform" title="Introducing Ansible Molecule with Ansible Automation Platform"&gt;Introducing Ansible Molecule with Ansible Automation Platform&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Anshul Behl</dc:creator><dc:date>2023-09-13T07:00:00Z</dc:date></entry></feed>
